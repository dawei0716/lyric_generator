{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yG_n40gFzf9s"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "# This line allows you to treat tf objects as np arrays.\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UHjdCjDuSvX_"
   },
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aavnuByVymwK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351917 characters\n",
      "92 unique characters\n"
     ]
    }
   ],
   "source": [
    "path_to_file = \"lyrics.txt\"\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "print(len(text), \"characters\")\n",
    "vocab = sorted(set(text))\n",
    "print(len(vocab), \"unique characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IalZLbvOzf-F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, '$': 4, '&': 5, \"'\": 6, '*': 7, '+': 8, ',': 9, '-': 10, '.': 11, '/': 12, '0': 13, '1': 14, '2': 15, '3': 16, '4': 17, '5': 18, '6': 19, '7': 20, '8': 21, '9': 22, ':': 23, ';': 24, '?': 25, 'A': 26, 'B': 27, 'C': 28, 'D': 29, 'E': 30, 'F': 31, 'G': 32, 'H': 33, 'I': 34, 'J': 35, 'K': 36, 'L': 37, 'M': 38, 'N': 39, 'O': 40, 'P': 41, 'Q': 42, 'R': 43, 'S': 44, 'T': 45, 'U': 46, 'V': 47, 'W': 48, 'X': 49, 'Y': 50, 'Z': 51, 'a': 52, 'b': 53, 'c': 54, 'd': 55, 'e': 56, 'f': 57, 'g': 58, 'h': 59, 'i': 60, 'j': 61, 'k': 62, 'l': 63, 'm': 64, 'n': 65, 'o': 66, 'p': 67, 'q': 68, 'r': 69, 's': 70, 't': 71, 'u': 72, 'v': 73, 'w': 74, 'x': 75, 'y': 76, 'z': 77, '{': 78, '}': 79, '~': 80, '¡': 81, 'à': 82, 'é': 83, 'ś': 84, '–': 85, '—': 86, '‘': 87, '’': 88, '“': 89, '”': 90, '…': 91}\n",
      "\n",
      "['\\n' ' ' '!' '\"' '$' '&' \"'\" '*' '+' ',' '-' '.' '/' '0' '1' '2' '3' '4'\n",
      " '5' '6' '7' '8' '9' ':' ';' '?' 'A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J'\n",
      " 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W' 'X' 'Y' 'Z' 'a' 'b'\n",
      " 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o' 'p' 'q' 'r' 's' 't'\n",
      " 'u' 'v' 'w' 'x' 'y' 'z' '{' '}' '~' '¡' 'à' 'é' 'ś' '–' '—' '‘' '’' '“'\n",
      " '”' '…']\n"
     ]
    }
   ],
   "source": [
    "#mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "\n",
    "# each ID can be the index\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "print(char2idx)\n",
    "print(\"\")\n",
    "print(idx2char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UNq_cpOwFRMq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's behind t\n",
      "[48 59 52 71  6 70  1 53 56 59 60 65 55  1 71]\n"
     ]
    }
   ],
   "source": [
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "\n",
    "# first 15 characters represented with integers.\n",
    "print(text[:15])\n",
    "print(text_as_int[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0UHJDA39zf-O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(48, shape=(), dtype=int64)\n",
      "W\n",
      "tf.Tensor(59, shape=(), dtype=int64)\n",
      "h\n",
      "tf.Tensor(52, shape=(), dtype=int64)\n",
      "a\n",
      "tf.Tensor(71, shape=(), dtype=int64)\n",
      "t\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "'\n"
     ]
    }
   ],
   "source": [
    "# convert text into a format that tensorflow can use.\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "# the first five things in the tensor stream. converts them to their char equivalents.\n",
    "for i in char_dataset.take(5):\n",
    "    print(i)\n",
    "    print(idx2char[i.numpy()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l4hkDU3i7ozi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"What's behind the other door? Oh-ohh\\nJust searching for the perfect shot\\n\\nWhen love comes calling, do\"\n",
      "\n",
      "\"n't look back\\nWhen love comes calling, don't look away\\nWhen love comes calling, don't look back\\nWhen \"\n",
      "\n",
      "\"love comes calling, don't look away\\n\\nI used to write rhymes all day and all night\\nWhen y'all was play\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# maximum length sentence (characters) to consider for a single input.\n",
    "seq_length = 100\n",
    "\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(3):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9NGu-FkO_kYU"
   },
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GNbw-iR0ymwj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  \"What's behind the other door? Oh-ohh\\nJust searching for the perfect shot\\n\\nWhen love comes calling, d\"\n",
      "Target data: \"hat's behind the other door? Oh-ohh\\nJust searching for the perfect shot\\n\\nWhen love comes calling, do\"\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0eBu9WZG84i0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 48 ('W')\n",
      "  expected output: 59 ('h')\n",
      "Step    1\n",
      "  input: 59 ('h')\n",
      "  expected output: 52 ('a')\n",
      "Step    2\n",
      "  input: 52 ('a')\n",
      "  expected output: 71 ('t')\n",
      "Step    3\n",
      "  input: 71 ('t')\n",
      "  expected output: 6 (\"'\")\n",
      "Step    4\n",
      "  input: 6 (\"'\")\n",
      "  expected output: 70 ('s')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p2pGotuNzf-S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: ((100,), (100,)), types: (tf.int64, tf.int64)>\n",
      "<DatasetV1Adapter shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "#dim before\n",
    "print(dataset)\n",
    "\n",
    "# Batch size: number of input examples to be processed together.\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset.\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "# dimensions after \n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r6oUuElIMgVx"
   },
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zHT8cLh7EAsg"
   },
   "outputs": [],
   "source": [
    "# hyperparameters.\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MtCrdfzEI2N0"
   },
   "outputs": [],
   "source": [
    "# Here's a function to construct a model.\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wwsrpOik5zhv"
   },
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C-_70kKAPrPU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 92) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vPGmAAXmVLGC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (64, None, 256)           23552     \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (64, None, 1024)          3935232   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (64, None, 92)            94300     \n",
      "=================================================================\n",
      "Total params: 4,053,084\n",
      "Trainable params: 4,053,084\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4V4MfFg0RQJg"
   },
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YqFMUQc_UFgM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([71,  9, 49, 83, 84, 43, 78, 26, 36, 70, 18, 30, 53,  3,  0, 71, 32,\n",
       "       67, 50, 42, 32, 56, 73, 80, 14, 17, 35, 14, 40, 89, 67, 10, 26, 50,\n",
       "       17,  8, 82, 68, 63, 45, 13, 19, 75, 63, 30, 38, 30, 59, 26, 82, 79,\n",
       "       40, 74, 65, 18, 88, 17, 71, 40, 47, 67, 60, 47, 77,  3, 33, 86, 24,\n",
       "       40, 79, 14, 39, 13,  7, 24, 41, 84, 44, 90, 30,  5, 34, 20, 28, 77,\n",
       "       43, 79,  6, 56, 57, 66, 77, 36, 35, 73, 12, 30, 20, 19, 52])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4HrXTACTdzY-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 92)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.5225954\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DDl1_Een6rL0"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ieSJdchZggUj"
   },
   "source": [
    "### Configure checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W6fWTriUZP-n"
   },
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Ky3F_BhgkTW"
   },
   "source": [
    "### Execute the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7yGBE2zxMMHs"
   },
   "outputs": [],
   "source": [
    "# We'll do 10 epochs of training. \n",
    "EPOCHS=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UK-hmKjYVoll"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "54/54 [==============================] - 218s 4s/step - loss: 2.6926\n",
      "Epoch 2/50\n",
      "54/54 [==============================] - 219s 4s/step - loss: 2.3344\n",
      "Epoch 3/50\n",
      "54/54 [==============================] - 218s 4s/step - loss: 2.1755\n",
      "Epoch 4/50\n",
      "54/54 [==============================] - 215s 4s/step - loss: 2.0384\n",
      "Epoch 5/50\n",
      "54/54 [==============================] - 216s 4s/step - loss: 1.9246\n",
      "Epoch 6/50\n",
      "54/54 [==============================] - 216s 4s/step - loss: 1.8266\n",
      "Epoch 7/50\n",
      "54/54 [==============================] - 216s 4s/step - loss: 1.7394\n",
      "Epoch 8/50\n",
      "54/54 [==============================] - 216s 4s/step - loss: 1.6613\n",
      "Epoch 9/50\n",
      "54/54 [==============================] - 217s 4s/step - loss: 1.5894\n",
      "Epoch 10/50\n",
      "54/54 [==============================] - 219s 4s/step - loss: 1.5209\n",
      "Epoch 11/50\n",
      "54/54 [==============================] - 216s 4s/step - loss: 1.4536\n",
      "Epoch 12/50\n",
      "54/54 [==============================] - 216s 4s/step - loss: 1.3859\n",
      "Epoch 13/50\n",
      "54/54 [==============================] - 216s 4s/step - loss: 1.3176\n",
      "Epoch 14/50\n",
      "54/54 [==============================] - 215s 4s/step - loss: 1.2505\n",
      "Epoch 15/50\n",
      "54/54 [==============================] - 216s 4s/step - loss: 1.1808\n",
      "Epoch 16/50\n",
      "54/54 [==============================] - 216s 4s/step - loss: 1.1139\n",
      "Epoch 17/50\n",
      "54/54 [==============================] - 217s 4s/step - loss: 1.0533\n",
      "Epoch 18/50\n",
      "54/54 [==============================] - 219s 4s/step - loss: 0.9855\n",
      "Epoch 19/50\n",
      "54/54 [==============================] - 216s 4s/step - loss: 0.9228\n",
      "Epoch 20/50\n",
      "54/54 [==============================] - 216s 4s/step - loss: 0.8555\n",
      "Epoch 21/50\n",
      "54/54 [==============================] - 216s 4s/step - loss: 0.7862\n",
      "Epoch 22/50\n",
      "54/54 [==============================] - 216s 4s/step - loss: 0.7223\n",
      "Epoch 23/50\n",
      "54/54 [==============================] - 217s 4s/step - loss: 0.6720\n",
      "Epoch 24/50\n",
      "54/54 [==============================] - 216s 4s/step - loss: 0.6364\n",
      "Epoch 25/50\n",
      "54/54 [==============================] - 214s 4s/step - loss: 0.6016\n",
      "Epoch 26/50\n",
      "54/54 [==============================] - 217s 4s/step - loss: 0.5531\n",
      "Epoch 27/50\n",
      "54/54 [==============================] - 215s 4s/step - loss: 0.5074\n",
      "Epoch 28/50\n",
      "54/54 [==============================] - 216s 4s/step - loss: 0.4637\n",
      "Epoch 29/50\n",
      "54/54 [==============================] - 215s 4s/step - loss: 0.4152\n",
      "Epoch 30/50\n",
      "54/54 [==============================] - 216s 4s/step - loss: 0.3687\n",
      "Epoch 31/50\n",
      "54/54 [==============================] - 215s 4s/step - loss: 0.3324\n",
      "Epoch 32/50\n",
      "54/54 [==============================] - 216s 4s/step - loss: 0.3063\n",
      "Epoch 33/50\n",
      "54/54 [==============================] - 216s 4s/step - loss: 0.2771\n",
      "Epoch 34/50\n",
      "54/54 [==============================] - 217s 4s/step - loss: 0.2509\n",
      "Epoch 35/50\n",
      "54/54 [==============================] - 216s 4s/step - loss: 0.2312\n",
      "Epoch 36/50\n",
      "54/54 [==============================] - 217s 4s/step - loss: 0.2090\n",
      "Epoch 37/50\n",
      "54/54 [==============================] - 216s 4s/step - loss: 0.1905\n",
      "Epoch 38/50\n",
      "54/54 [==============================] - 216s 4s/step - loss: 0.1747\n",
      "Epoch 39/50\n",
      "54/54 [==============================] - 216s 4s/step - loss: 0.1615\n",
      "Epoch 40/50\n",
      "54/54 [==============================] - 216s 4s/step - loss: 0.1502\n",
      "Epoch 41/50\n",
      "54/54 [==============================] - 216s 4s/step - loss: 0.1380\n",
      "Epoch 42/50\n",
      "54/54 [==============================] - 217s 4s/step - loss: 0.1271\n",
      "Epoch 43/50\n",
      "54/54 [==============================] - 219s 4s/step - loss: 0.1193\n",
      "Epoch 44/50\n",
      "54/54 [==============================] - 217s 4s/step - loss: 0.1111\n",
      "Epoch 45/50\n",
      "54/54 [==============================] - 216s 4s/step - loss: 0.1014\n",
      "Epoch 46/50\n",
      "54/54 [==============================] - 216s 4s/step - loss: 0.0934\n",
      "Epoch 47/50\n",
      "54/54 [==============================] - 217s 4s/step - loss: 0.0856\n",
      "Epoch 48/50\n",
      "54/54 [==============================] - 216s 4s/step - loss: 0.0764\n",
      "Epoch 49/50\n",
      "54/54 [==============================] - 217s 4s/step - loss: 0.0680\n",
      "Epoch 50/50\n",
      "54/54 [==============================] - 217s 4s/step - loss: 0.0621\n",
      "training complete!\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
    "print(\"training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kKkD5M6eoSiN"
   },
   "source": [
    "## Part 4: Generating text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LycQ-ot_jjyu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./training_checkpoints/ckpt_50\n"
     ]
    }
   ],
   "source": [
    "print(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "# instantiate a new model with the desired parameters and hyperparameters\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "# model.load_weights('./training_checkpoints/ckpt_100')\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "71xa6jnYVrAN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (1, None, 256)            23552     \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (1, None, 1024)           3935232   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (1, None, 92)             94300     \n",
      "=================================================================\n",
      "Total params: 4,053,084\n",
      "Trainable params: 4,053,084\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WvuwZBX5Ogfd"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "\n",
    "  # Number of characters to generate.\n",
    "  # You can change this to whatever you like, of course.\n",
    "  num_generate = 400\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 1.0\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the word returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ktovv0RFhrkn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorry\n",
      "\n",
      "Everybody wanna cut the legs off him\n",
      "\n",
      "Telax I live a out a plate, that pussy and Patron\n",
      "Give me a run for my money\n",
      "There is nobody, no one to outrun me\n",
      "So give me a run for myself\n",
      "A misa've seen to fld that I'm try to fight my simplest legs\n",
      "And I'm gon' shine like a ned, think abe I want to be free\n",
      "When you know that lines from 'em\n",
      "See too many of y'all getting that lioe sky I to make a promise \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"sorry\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zpAFJG87FRN2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lab7.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
